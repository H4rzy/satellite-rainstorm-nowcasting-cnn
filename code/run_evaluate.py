import os
import numpy as np
import torch
import torch.nn as nn
from torchvision import models
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend ƒë·ªÉ save file

from dataset import SatelliteDataset
from imagetransform import ImageTransform

def evaluate_and_save(model, dataloader, device, class_names, save_path):
    """Evaluate model v√† l∆∞u confusion matrix ra file PNG"""
    model.eval()
    all_preds = []
    all_labels = []
    
    print("ƒêang ƒë√°nh gi√° model...")
    with torch.no_grad():
        for imgs, labels in dataloader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            preds = torch.argmax(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # T·∫°o confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    
    # V·∫Ω Confusion Matrix
    fig, ax = plt.subplots(figsize=(10, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')
    plt.title("Confusion Matrix - Rain Prediction Model", fontsize=14, fontweight='bold')
    plt.xlabel("D·ª± ƒëo√°n (Predicted)", fontsize=12)
    plt.ylabel("Th·ª±c t·∫ø (Actual)", fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ ƒê√£ l∆∞u Confusion Matrix: {save_path}")
    plt.close()
    
    return cm, all_labels, all_preds

def calculate_metrics(cm, class_names):
    """T√≠nh v√† in c√°c metrics"""
    num_classes = len(class_names)
    precision = np.zeros(num_classes)
    recall = np.zeros(num_classes)
    f1 = np.zeros(num_classes)

    for i in range(num_classes):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        precision[i] = tp / (tp + fp + 1e-8)
        recall[i] = tp / (tp + fn + 1e-8)
        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i] + 1e-8)

    # T·ªïng accuracy
    accuracy = np.trace(cm) / cm.sum() * 100
    
    print("\n" + "="*60)
    print("ƒê√ÅNH GI√Å MODEL - RAIN PREDICTION")
    print("="*60)
    print(f"\nüìä T·ªîNG QUAN:")
    print(f"   Accuracy: {accuracy:.2f}%")
    print(f"   T·ªïng m·∫´u test: {cm.sum()}")
    
    print(f"\nüìà CHI TI·∫æT T·ª™NG L·ªöP:")
    print("-"*60)
    for name, p, r, f in zip(class_names, precision, recall, f1):
        print(f"   {name:12s} | Precision: {p*100:6.2f}% | Recall: {r*100:6.2f}% | F1: {f*100:6.2f}%")
    
    print("-"*60)
    print(f"   Macro Avg     | Precision: {precision.mean()*100:6.2f}% | Recall: {recall.mean()*100:6.2f}% | F1: {f1.mean()*100:6.2f}%")
    print("="*60)
    
    return accuracy, precision, recall, f1

def plot_metrics_bar(class_names, precision, recall, f1, save_path):
    """V·∫Ω bi·ªÉu ƒë·ªì bar cho metrics"""
    x = np.arange(len(class_names))
    width = 0.25

    fig, ax = plt.subplots(figsize=(12, 6))
    bars1 = ax.bar(x - width, precision * 100, width, label='Precision', color='#2196F3')
    bars2 = ax.bar(x, recall * 100, width, label='Recall', color='#4CAF50')
    bars3 = ax.bar(x + width, f1 * 100, width, label='F1-score', color='#FF9800')

    ax.set_xlabel('L·ªõp d·ª± ƒëo√°n', fontsize=12)
    ax.set_ylabel('Ph·∫ßn trƒÉm (%)', fontsize=12)
    ax.set_title('Precision / Recall / F1-score theo t·ª´ng l·ªõp', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(class_names, fontsize=11)
    ax.legend(fontsize=11)
    ax.grid(axis='y', linestyle='--', alpha=0.3)
    ax.set_ylim(0, 100)

    # Th√™m gi√° tr·ªã tr√™n bar
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.1f}%', xy=(bar.get_x() + bar.get_width()/2, height),
                       xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì Metrics: {save_path}")
    plt.close()

def main():
    # C·∫•u h√¨nh
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è  Device: {device}")
    
    class_names = ['not_rain', 'medium_rain', 'heavy_rain']
    
    # Load model
    print("üì¶ ƒêang load model...")
    model = models.resnet34(weights=None)
    num_features = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Dropout(p=0.6),
        nn.Linear(num_features, 3)
    )
    
    # Th·ª≠ load model t·ª´ c√°c path c√≥ th·ªÉ
    model_paths = [
        "model/model_076.pth",
        "model/model_082.pth",
        "model/satellite_model.pth"
    ]
    
    model_loaded = False
    for path in model_paths:
        if os.path.exists(path):
            print(f"   Loading from: {path}")
            state_dict = torch.load(path, map_location=device)
            model.load_state_dict(state_dict)
            model_loaded = True
            break
    
    if not model_loaded:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file model! Vui l√≤ng ki·ªÉm tra l·∫°i.")
        print("   C√°c ƒë∆∞·ªùng d·∫´n ƒë√£ th·ª≠:", model_paths)
        return
    
    model = model.to(device)
    model.eval()
    print("‚úÖ Model loaded th√†nh c√¥ng!")
    
    # Load dataset
    print("\nüìÇ ƒêang load dataset...")
    test_dir = 'Data/val'  # D√πng validation set ƒë·ªÉ test
    if not os.path.exists(test_dir):
        test_dir = 'Data/train'
        print(f"   D√πng {test_dir} ƒë·ªÉ ƒë√°nh gi√°")
    
    resize = 224
    mean = (0.485, 0.456, 0.406)
    std = (0.229, 0.224, 0.225)
    transform = ImageTransform(resize, mean, std)
    
    test_dataset = SatelliteDataset(test_dir, transform=transform.data_transform['val'])
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    print(f"‚úÖ Dataset loaded: {len(test_dataset)} m·∫´u")
    
    # Evaluate v√† l∆∞u confusion matrix
    cm, labels, preds = evaluate_and_save(
        model, test_loader, device, class_names, 
        save_path="confusion_matrix_result.png"
    )
    
    # T√≠nh metrics
    accuracy, precision, recall, f1 = calculate_metrics(cm, class_names)
    
    # V·∫Ω bi·ªÉu ƒë·ªì metrics
    plot_metrics_bar(class_names, precision, recall, f1, 
                     save_path="metrics_bar_chart.png")
    
    print("\n" + "="*60)
    print("üéâ HO√ÄN T·∫§T! C√°c file ƒë√£ t·∫°o:")
    print("   - confusion_matrix_result.png")
    print("   - metrics_bar_chart.png")
    print("="*60)

if __name__ == "__main__":
    main()
